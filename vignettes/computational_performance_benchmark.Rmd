---
title: "Performance Benchmark: rLifting vs CRAN"
output: 
  rmarkdown::html_vignette:
    css: style.css
    toc: true
    toc_depth: 2
vignette: >
  %\VignetteIndexEntry{Benchmark de Performance Computacional}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE,
  message = FALSE,
  fig.width = 7, 
  fig.height = 5
)
```

`rLifting` was designed with a primary goal: **High-Performance Signal Processing**.
While most R wavelet packages focus on offline statistical analysis (post-processing), this package offers robust solutions for both offline analysis and high-throughput streaming.

## The Landscape of Lifting in R

Before benchmarking, it is important to address the "elephant in the room".
There are other modern packages for Lifting in R, notably `adlift` and `nlt`.
Why aren't we comparing against them?

-   **Different Objectives:**

    -   `adlift` focuses on **irregular grids** and performs complex adaptive prediction steps per point.
        It is statistically powerful for difficult data but computationally heavy.

    -   `nlt` implements the **Non-Decimated** transform (redundant), which improves denoising but consumes significantly more memory and CPU.

    -   `rLifting` targets **regular grids** (standard sensors/audio) and prioritizes **execution speed** via a C++ Ring Buffer.

Comparing `rLifting` to `adlift` would be unfair to `adlift`, as it solves a much harder mathematical problem.
Therefore, we benchmark against `wavethresh`, which is the standard for classic DWT/Lifting on regular grids.

## Benchmark Setup

We simulate a small-load scenario:

-   **N = 50** independent signals.

-   **T = 512** samples per signal.

-   **Wavelet:** Haar.

-   **Levels:** 3 decomposition levels.

```{r}
library(rLifting)
library(microbenchmark)
library(ggplot2)
library(dplyr)

theme_set(theme_bw())
# Try to load competitors
has_wavethresh = requireNamespace("wavethresh", quietly = TRUE)

# TEST DATA
set.seed(42)
N_SIGNALS = 50
T_LEN = 512
signals = matrix(rnorm(N_SIGNALS * T_LEN), nrow = T_LEN, ncol = N_SIGNALS)

# Setup rLifting
scheme_rl = lifting_scheme("haar")
```

## Scenario 1: Offline Denoising (Batch)

In this scenario, all data is available.
The goal is to read the signal, decompose, threshold (soft), and reconstruct.

```{r}
if (has_wavethresh) {
  # Wrapper for Wavethresh (WD -> Threshold -> WR)
  run_wavethresh_batch = function(mat) {
    apply(mat, 2, function(x) {
      w = wavethresh::wd(
        x, family = "DaubExPhase", 
        filter.number = 1, bc = "symmetric"
        )
      w_t = wavethresh::threshold(
        w, policy = "universal", type = "soft", 
        dev = mad, verbose = FALSE
        )
      wavethresh::wr(w_t)
    })
  }
}

# Wrapper rLifting
run_rlifting_batch = function(mat) {
  # Level 3, Haar, Soft, Symmetric
  # Apply loop is in R, but the core is C++
  apply(mat, 2, function(x) {
    denoise_signal_offline(
      x, scheme_rl, levels = 3, 
      method = "soft", extension = "symmetric"
    )
  })
}

# Running Benchmark (Only if wavethresh exists)
if (has_wavethresh) {
  res_offline = microbenchmark(
    wavethresh = run_wavethresh_batch(signals),
    rLifting = run_rlifting_batch(signals),
    times = 50
  )
  
  print(res_offline)
  autoplot(res_offline) + 
    labs(title = "Offline Denoising (50 signals x 512 points)")
} else {
  cat("Package 'wavethresh' not installed. Skipping offline comparison.")
}
```

`rLifting` uses a C++ engine that performs decomposition, thresholding, and reconstruction in a single memory pass ("Zero-Allocation"), avoiding the creation of complex intermediate S3/S4 objects.
This generally results in highly competitive or superior performance, even in offline mode.

## Scenario 2: Online Denoising (Real-Time)

This is the package's key differentiator.
Imagine receiving **one new data point** every millisecond.

-   **Traditional Approach (Naive Sliding Window):** Since classic packages are stateless (no memory), you must take the last 1024 points, run the full DWT, filter, run IDWT, and pick the last point.
    This is $O(N)$ per step.

-   **rLifting Approach (Causal Ring Buffer):** The package maintains internal state in C++.
    Upon receiving a point, it updates only the necessary coefficients via the Lifting Scheme.
    This effectively results in **constant time complexity per sample relative to stream length**, maintaining low latency.

Let's simulate processing just 30 steps of a stream.

```{r}
# Sample signal
stream_data = signals[,1]
new_points = rnorm(30) # 60 new incoming points

# Traditional Approach
run_naive_sliding = function() {
  buffer = stream_data # Initial buffer
  out = numeric(30)
  for(i in 1:30) {
    # Add point, remove old (Fixed Window)
    buffer = c(buffer[-1], new_points[i]) 
    
    # Reprocess EVERYTHING
    w = wavethresh::wd(
      buffer, family="DaubExPhase", 
      filter.number=1, bc="symmetric"
      )
    w_t = wavethresh::threshold(
      w, policy="universal", 
      type="soft", dev=mad, verbose=FALSE
      )
    rec = wavethresh::wr(w_t)
    
    out[i] = tail(rec, 1) # Pick only the last one
  }
  return(out)
}

# rLifting Approach (Stateful)
run_rlifting_stream = function() {
  # Create persistent engine (C++ XPtr)
  proc = new_wavelet_stream(
    scheme_rl, window_size = 512, levels = 3, 
    method = "soft"
  )
  
  # Warm up with history (fast simulation)
  # State is preserved in C++
  invisible(sapply(stream_data, proc)) 
  
  # Process 50 new points
  sapply(new_points, proc)
}

if (has_wavethresh) {
  res_online = microbenchmark(
    Traditional_Sliding = run_naive_sliding(),
    rLifting_Stream = run_rlifting_stream(),
    times = 50
  )
  
  print(res_online)
  
  # Log Scale Visualization (Difference is massive)
  autoplot(res_online) + 
    scale_y_log10() +
    labs(
      title = "Online Processing (30 steps)", 
      subtitle = "Log Scale! rLifting is orders of magnitude faster."
    )
}
```

## Scenario 3: Throughput (Zero Latency)

For High-Frequency Trading (HFT) or IoT sensors, latency per sample matters.
Here we measure the time to inject a *single sample* into the C++ engine.

```{r}
# Create light engine
fast_proc = new_wavelet_stream(scheme_rl, window_size = 256, levels = 3)

# Measure single sample injection time
res_latency = microbenchmark(
  Single_Sample_Push = fast_proc(0.5),
  times = 1000
)

print(res_latency)

avg_time_ns = summary(res_latency)$mean * 1000 # convert to nanoseconds
cat(
  sprintf(
    "\nAverage time per sample: %.2f microseconds\n", avg_time_ns / 1000
    )
  )
cat(
  sprintf(
    "Theoretical Capacity: %.0f events/second\n", 1e9 / avg_time_ns * 1000
    )
  )
```

## Conclusion

1.  **Offline:** `rLifting` is a modern, high-performance alternative to traditional packages for regular grids.

2.  **Online:** `rLifting` enables analyses that are computationally infeasible with stateless packages.
    The Ring Buffer architecture removes the $O(N)$ bottleneck.

3.  **Latency:** With microsecond-level latency, the package is suitable for high-throughput systems.
